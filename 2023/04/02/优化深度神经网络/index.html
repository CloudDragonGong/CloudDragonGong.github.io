<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本片笔记从是吴恩达教授的学习笔记，以及配套的博客梳理，方便巩固复习和完成编程作业。">
<meta property="og:type" content="article">
<meta property="og:title" content="优化深度神经网络">
<meta property="og:url" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="云龙博客">
<meta property="og:description" content="本片笔记从是吴恩达教授的学习笔记，以及配套的博客梳理，方便巩固复习和完成编程作业。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411195946029.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411200328890.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411203152258.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411210832437.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411210851178.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411211306029.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411211318770.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411232643691.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411234454503.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411234651570.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411234707571.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411235435598.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412102844661.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412102853117.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412104055472.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412104118270.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412160212366.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412160329067.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412160341046.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412161524341.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412161534208.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412161542195.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412161940206.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412162308435.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412162325776.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412162332885.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412162342006.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412162454191.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412162701714.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412162722004.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412162746241.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412164156193.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412164207320.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412164258943.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412165251889.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412165304120.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412165323407.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412165635202.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412165858036.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412165920307.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412165927991.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412170242234.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413170911744.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413171238636.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413172548338.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413174215113.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413174319016.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413174348082.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413175009660.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413182643567.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413182909608.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413183546134.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413184010280.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413184018474.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413184028303.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413184046074.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413190807045.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413190941182.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413191050896.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413191103844.png">
<meta property="article:published_time" content="2023-04-02T09:51:35.000Z">
<meta property="article:modified_time" content="2023-04-13T14:31:27.902Z">
<meta property="article:author" content="DragonGong">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411195946029.png">

<link rel="canonical" href="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>优化深度神经网络 | 云龙博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">云龙博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="DragonGong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云龙博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          优化深度神经网络
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-04-02 17:51:35" itemprop="dateCreated datePublished" datetime="2023-04-02T17:51:35+08:00">2023-04-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-13 22:31:27" itemprop="dateModified" datetime="2023-04-13T22:31:27+08:00">2023-04-13</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">吴恩达深度学习笔记</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>9.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>本片笔记从是<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1FT4y1E74V/?spm_id_from=333.999.0.0">吴恩达教授</a>的学习笔记，以及配套的<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_44543774/article/details/119154088?spm=1001.2014.3001.5506">博客</a>梳理，方便巩固复习和完成编程作业。</p>
</blockquote>
<span id="more"></span>

<h2 id="深度学习的实用层面"><a href="#深度学习的实用层面" class="headerlink" title="深度学习的实用层面"></a>深度学习的实用层面</h2><h3 id="Train-x2F-Dev-x2F-Test-sets"><a href="#Train-x2F-Dev-x2F-Test-sets" class="headerlink" title="Train&#x2F;Dev&#x2F;Test sets"></a>Train&#x2F;Dev&#x2F;Test sets</h3><blockquote>
<p>训练集（Training sets）、验证集（Development sets）、测试集（Test sets）</p>
</blockquote>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411195946029.png" alt="image-20230411195946029" style="zoom:50%;">

<p>样本数据分成三个部分：Train&#x2F;Dev&#x2F;Test sets</p>
<ul>
<li>本数量不是很大的情况下，例如100,1000,10000,Train&#x2F;Dev&#x2F;Test sets的比例通常可以设置为比例为<code>60%、20%、20%</code></li>
<li>大数据样本，Train&#x2F;Dev&#x2F;Test sets的比例通常可以设置为<code>98%/1%/1%</code></li>
</ul>
<h3 id="Bias-x2F-Variance-偏差（Bias）和方差（Variance）"><a href="#Bias-x2F-Variance-偏差（Bias）和方差（Variance）" class="headerlink" title="Bias&#x2F;Variance(偏差（Bias）和方差（Variance）)"></a>Bias&#x2F;Variance(偏差（Bias）和方差（Variance）)</h3><img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411200328890.png" alt="image-20230411200328890" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411203152258.png" alt="image-20230411203152258" style="zoom: 33%;">

<ul>
<li>Train set error为1%，而Dev set error为11%–&gt;过拟合</li>
<li>Train set error为15%，而Dev set error为16% –&gt; 欠拟合</li>
<li>Train set error为15%，而Dev set error为30% –&gt; 又过拟合又欠拟合（最坏的情况）</li>
<li>假设Train set error为0.5%，而Dev set error为1% –&gt; 最好的情况</li>
</ul>
<blockquote>
<p>模型既存在high bias也存在high variance，可以理解成某段区域是欠拟合的，某段区域是过拟合的</p>
</blockquote>
<h3 id="Basic-Recipe-for-Machine-Learning"><a href="#Basic-Recipe-for-Machine-Learning" class="headerlink" title="Basic Recipe for Machine Learning"></a>Basic Recipe for Machine Learning</h3><ul>
<li><p>如何解决欠拟合</p>
<blockquote>
<p>增加神经网络的层数和节点数、选择更复杂的模型、增加训练时间等方式</p>
</blockquote>
</li>
<li><p>如何解决过拟合</p>
<blockquote>
<p>加训练集数据、使用正则化技术等方式</p>
</blockquote>
</li>
</ul>
<p>为什么深度学习如此强大：</p>
<blockquote>
<p>传统机器学习算法中偏差和方差通常是对立的，减小偏差会增加方差，减小方差会增加偏差。</p>
<p>但是在深度学习中，通过使用更复杂的神经网络和大量的训练数据，我们一般可以同时减小偏差和方差</p>
</blockquote>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><h4 id="L1和L2范数"><a href="#L1和L2范数" class="headerlink" title="L1和L2范数"></a>L1和L2范数</h4><p>L1范数和L2范数是衡量向量大小的方式。</p>
<p>L1范数，也称为曼哈顿距离（Manhattan distance），是指向量各个元素绝对值之和。对于一个n维向量x，它的L1范数为 ||x||1 &#x3D; |x1| + |x2| + … + |xn|。</p>
<p>L2范数，也称为欧几里得距离（Euclidean distance），是指向量各个元素的平方和再开平方根。对于一个n维向量x，它的L2范数为 ||x||2 &#x3D; sqrt(x1^2 + x2^2 + … + xn^2)。</p>
<p>L1范数和L2范数都可以用于正则化（regularization）技术中，其中L1正则化通常会导致模型的系数变得稀疏（即很多系数为0），而L2正则化则会让模型的系数尽可能地小，但不会稀疏。在神经网络中，L2正则化也被称为权重衰减（weight decay）。</p>
<h4 id="正则化参数-lambda"><a href="#正则化参数-lambda" class="headerlink" title="正则化参数$\lambda$"></a>正则化参数$\lambda$</h4><p>在正则化中，超参数λ（也称为正则化参数或惩罚因子）是用于控制正则化强度的调整参数。较大的λ值会导致更强的正则化，可以有效减少模型的过拟合问题，但可能会增加模型的欠拟合问题。较小的λ值会导致较弱的正则化，使模型更加复杂，可能会导致过拟合问题</p>
<p>通过调整λ的值来达到合适的正则化强度，一般采用<em>交叉验证</em>的方法来确定最佳的λ值</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411210832437.png" alt="image-20230411210832437" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411210851178.png" alt="image-20230411210851178" style="zoom:50%;">

<h3 id="Why-regularization-reduces-overfitting"><a href="#Why-regularization-reduces-overfitting" class="headerlink" title="Why regularization reduces overfitting"></a><strong>Why regularization reduces overfitting</strong></h3><img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411211306029.png" alt="image-20230411211306029" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411211318770.png" alt="image-20230411211318770" style="zoom:50%;">

<blockquote>
<p>正则化能够减少过拟合的原因是，它通过对模型的参数进行约束，降低了模型的复杂度，从而减少了模型在训练数据上的误差，并提高了模型的泛化能力。具体来说，L1和L2正则化会对模型的参数进行惩罚，使得模型在优化过程中更倾向于选择较小的参数值，从而达到减小模型复杂度的效果。通过减小模型的复杂度，模型在验证数据上的表现也会得到提升，从而有效避免过拟合的问题。</p>
</blockquote>
<h3 id="Dropout-Regularization"><a href="#Dropout-Regularization" class="headerlink" title="Dropout Regularization"></a><strong>Dropout Regularization</strong></h3><ul>
<li><p>什么是<strong>Dropout Regularization</strong></p>
<blockquote>
<p>Dropout是指在深度学习网络的训练过程中，对于每层的神经元，按照一定的概率将其暂时从网络中丢弃。也就是说，每次训练时，每一层都有部分神经元不工作，起到简化复杂网络模型的效果，从而避免发生过拟合。</p>
</blockquote>
</li>
</ul>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411232643691.png" alt="image-20230411232643691" style="zoom:50%;">

<ul>
<li><p>如何实现</p>
<blockquote>
<p>Dropout有不同的实现方法，接下来介绍一种常用的方法：Inverted dropout。假设对于第$l$层神经元，设定保留神经元比例概率keep_prob&#x3D;0.8，即该层有20%的神经元停止工作。$dl$为dropout向量，设置$dl$为随机vector，其中80%的元素为1，20%的元素为0</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dl = np.random.rand(al.shape[<span class="number">0</span>],al.shape[<span class="number">1</span>])&lt;keep_prob</span><br><span class="line">al = np.multiply(al,dl)</span><br><span class="line">al /= keep_prob <span class="comment"># scale up</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>什么是scale up</p>
<blockquote>
<p>在Inverted dropout中，我们对某一层的神经元进行了随机的丢弃操作，即将某些神经元的输出值设为0。为了避免丢弃操作对网络的输出造成过大的影响，我们需要对剩下的神经元的输出值进行调整，即将剩下的神经元的输出值除以保留概率（即1-丢弃概率）。这个调整过程就叫做scale up（缩放）操作。</p>
<p>举个例子，如果我们对某一层的神经元进行了50%的丢弃，即丢弃概率为0.5，那么保留概率就是0.5，我们需要对剩下的神经元的输出值进行调整，即将其除以0.5，使其尽量保持不变。这样，在经过dropout后，剩下的神经元的输出值的均值和方差与没有进行dropout的情况下差不多，这有助于减轻dropout对网络输出的影响，从而提高网络的泛化能力。</p>
</blockquote>
</li>
<li><p>使用dropout训练结束后，在测试和实际应用模型时，<strong>不需要进行dropout和随机删减神经元</strong>，所有的神经元都在工作</p>
</li>
</ul>
<h3 id="Understanding-Dropout"><a href="#Understanding-Dropout" class="headerlink" title="Understanding Dropout"></a><strong>Understanding Dropout</strong></h3><ul>
<li><p>keep_prob 和 keep_out </p>
<blockquote>
<p>指代的是 dropout 算法中的概率值，表示神经元保留的概率，即被保留下来的概率。在实际使用中，keep_prob 和 keep_out 可以看作是同一个概念，表示的是相同的概率值。只是在不同的文献和资料中，可能使用的术语不同</p>
</blockquote>
</li>
<li><p>不同的隐藏层的dropout系数keep_prob可以不同。</p>
</li>
<li><p>神经元越多的隐藏层，keep_prob可以设置的小一些，例如0.5。</p>
</li>
<li><p>神经元越少的隐藏层，keep_prob可以设置的大一些，例如0.8，设置是1。</p>
</li>
<li><p>不建议对输入层进行dropout，如果输入层维度很大，例如图片，那么可以设置dropout，但keep_prob应设置的大一些，例如0.8，0.9。</p>
</li>
<li><p>越容易出现overfitting的隐藏层，其keep_prob就设置的相对小一些。</p>
</li>
<li><p>没有固定的做法，通常可以根据validation进行选择。# validation–验证，实验</p>
</li>
</ul>
<h3 id="Other-regularization-methods"><a href="#Other-regularization-methods" class="headerlink" title="Other regularization methods"></a><strong>Other regularization methods</strong></h3><h4 id="L2-regularization和dropout-regularization以外的方法"><a href="#L2-regularization和dropout-regularization以外的方法" class="headerlink" title="L2 regularization和dropout regularization以外的方法"></a>L2 regularization和dropout regularization以外的方法</h4><ul>
<li><p>增加训练样本数量，但通常成本较高，因此可以使用数据增强技术来制造更多样本，如图片翻转、旋转、缩放等，这些都能防止过拟合。</p>
</li>
<li><p>在数字识别中，也可以对原有图片进行任意旋转、扭曲或者添加噪声等操作。</p>
</li>
<li><p>另一种防止过拟合的方法是early stopping，即在训练过程中根据train set error和dev set error的变化趋势来选择合适的迭代次数，以避免过拟合。</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411234454503.png" alt="image-20230411234454503" style="zoom:50%;"></li>
</ul>
<h4 id="Early-stopping的缺点"><a href="#Early-stopping的缺点" class="headerlink" title="Early stopping的缺点"></a>Early stopping的缺点</h4><ul>
<li>Early stopping通过减少训练次数来防止过拟合，与优化cost function的目标彼此对立，缺乏正交化。</li>
<li>Early stopping将上述两个目标融合在一起，同时优化，但可能没有“分而治之”的效果好。</li>
</ul>
<h4 id="L2-regularization相比early-stopping的优点"><a href="#L2-regularization相比early-stopping的优点" class="headerlink" title="L2 regularization相比early stopping的优点"></a>L2 regularization相比early stopping的优点</h4><ul>
<li>L2 regularization可以实现“分而治之”的效果：迭代训练足够多，减小cost function，同时也能有效防止过拟合。</li>
<li>L2 regularization的缺点之一是最优的正则化参数λ的选择比较复杂。</li>
<li>Early stopping相比L2 regularization比较简单，但L2 regularization更加常用一些。</li>
</ul>
<h3 id="Normalizing-inputs（归一化输入）"><a href="#Normalizing-inputs（归一化输入）" class="headerlink" title="Normalizing inputs（归一化输入）"></a><strong>Normalizing inputs</strong>（归一化输入）</h3><p>在训练神经网络时，标准化输入可以提高训练的速度。标准化输入就是对训练数据集进行归一化的操作，即将原始数据减去其均值$μ$后，再除以其方差$σ$2：</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411234651570.png" alt="image-20230411234651570" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411234707571.png" alt="image-20230411234707571" style="zoom:50%;">

<ol>
<li>标准化输入可以提高神经网络训练的速度和准确度。</li>
<li>标准化输入是对训练数据集进行归一化的操作，即将原始数据减去其均值后，再除以其方差。</li>
<li>在训练集进行标准化处理后，测试集或在实际应用时，应使用同样的均值和方差进行标准化处理，保证训练集和测试集的标准化操作一致。</li>
<li>标准化输入的主要目的是让所有输入在同样的尺度上归一化，方便进行梯度下降算法时能够更快更准确地找到全局最优解。</li>
<li>标准化处理可以让特征分布均匀，从而使得得到的cost function与权重和偏置的关系是类似圆形碗，便于梯度下降算法的优化。</li>
<li>如果输入特征之间的范围本来就比较接近，那么不进行标准化操作也是没有太大影响的，但标准化处理在大多数场合下还是值得推荐的</li>
</ol>
<h3 id="Vanishing-and-Exploding-gradients-梯度消失"><a href="#Vanishing-and-Exploding-gradients-梯度消失" class="headerlink" title="Vanishing and Exploding gradients(梯度消失)"></a><strong>Vanishing and Exploding gradients</strong>(梯度消失)</h3><img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230411235435598.png" alt="image-20230411235435598" style="zoom:50%;">

<p>深度神经网络中存在梯度消失和梯度爆炸问题，导致训练过程困难。当各层权重都大于1或小于1时，激活函数的输出会随着层数的增加呈指数型增大或减小，出现数值爆炸或消失。同样，梯度也会出现同样的指数型增大或减小的变化，当网络层数很大时，如L&#x3D;150，则梯度会非常大或非常小，引起每次更新的步进长度过大或者过小，让训练过程十分困难。</p>
<h3 id="Weight-Initialization-for-Deep-Networks"><a href="#Weight-Initialization-for-Deep-Networks" class="headerlink" title="Weight Initialization for Deep Networks"></a><strong>Weight Initialization for Deep Networks</strong></h3><p>在深度神经网络中，初始化权重的方法对于网络的训练非常重要。在神经网络中，权重矩阵W的大小决定了神经元输出Z的大小。如果W过大或过小，神经元输出Z就容易出现梯度消失或梯度爆炸的问题，导致网络难以收敛。</p>
<p>针对这个问题，可以通过调整权重矩阵W的初始化方法来解决。</p>
<ul>
<li><p>对于使用tanh作为激活函数的网络，通常建议使用方差为1&#x2F;n的初始化方法；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l-<span class="number">1</span>])*np.sqrt(<span class="number">1</span>/n[l-<span class="number">1</span>]) </span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络结构</span></span><br><span class="line">layer_dims = [<span class="number">4</span>, <span class="number">5</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化权重</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_weights</span>(<span class="params">layer_dims</span>):</span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    L = <span class="built_in">len</span>(layer_dims)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)] = np.random.randn(layer_dims[l], layer_dims[l-<span class="number">1</span>]) * np.sqrt(<span class="number">1</span>/layer_dims[l-<span class="number">1</span>])</span><br><span class="line">        parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用函数进行初始化</span></span><br><span class="line">parameters = initialize_weights(layer_dims)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看初始化后的权重</span></span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> parameters.items():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;W&#x27;</span> <span class="keyword">in</span> key:</span><br><span class="line">        <span class="built_in">print</span>(key + <span class="string">&quot; shape: &quot;</span> + <span class="built_in">str</span>(value.shape))</span><br><span class="line">        <span class="built_in">print</span>(key + <span class="string">&quot; value: &quot;</span> + <span class="built_in">str</span>(value))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">W1 shape: (5, 4)</span><br><span class="line">W1 value: [[ 1.78862847  0.43650985  0.09649747 -1.8634927 ]</span><br><span class="line"> [-0.2773882  -0.35475898 -0.08274148 -0.62700068]</span><br><span class="line"> [-0.04381817 -0.47721803 -1.31386475   0.88462238]</span><br><span class="line"> [ 0.88131804  1.70957306  0.05003364 -0.40467741]</span><br><span class="line"> [-0.54535995 -1.54647732  0.98236743 -1.10106763]]</span><br><span class="line">W2 shape: (3, 5)</span><br><span class="line">W2 value: [[-0.1382643   0.20345524  0.75878541 -0.22404411  0.22863013]</span><br><span class="line"> [ 0.44513761 -0.37598474 -0.21611069 -0.30847027 -0.10917586]</span><br><span class="line"> [ 0.90082649  0.46566244 -1.53624369  1.48825219  1.89588918]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>


</li>
<li><p>对于使用ReLU作为激活函数的网络，建议使用方差为2&#x2F;n的初始化方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l-<span class="number">1</span>])*np.sqrt(<span class="number">2</span>/n[l-<span class="number">1</span>]) </span><br></pre></td></tr></table></figure>


</li>
<li><p>此外，Yoshua Bengio提出了一种初始化方法，可以设置权重的方差为2&#x2F;n[l-1]*n[l]，其中n[l-1]和n[l]分别是前一层和当前层的神经元数量。不同的初始化方法适用于不同的网络结构和激活函数，选择哪种方法要根据实际情况进行调整。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l-<span class="number">1</span>])*np.sqrt(<span class="number">2</span>/n[l-<span class="number">1</span>]*n[l]) </span><br></pre></td></tr></table></figure></li>
</ul>
<p>在实际应用中，为了获得最优的初始化方法，可以通过验证集对超参数进行调整。</p>
<h3 id="Numerical-approximation-of-gradients"><a href="#Numerical-approximation-of-gradients" class="headerlink" title="Numerical approximation of gradients"></a><strong>Numerical approximation of gradients</strong></h3><img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412102844661.png" alt="image-20230412102844661" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412102853117.png" alt="image-20230412102853117" style="zoom:50%;">

<h3 id="Gradient-checking"><a href="#Gradient-checking" class="headerlink" title="Gradient checking"></a><strong>Gradient checking</strong></h3><p>检查步骤</p>
<ul>
<li><p>计算$J(\theta)$</p>
</li>
<li><p>计算$d\theta$:通过反向传播来计算</p>
</li>
<li><p>计算估计的$d\theta$</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412104055472.png" alt="image-20230412104055472" style="zoom:50%;">
</li>
<li><p>计算欧氏距离比较：</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412104118270.png" alt="image-20230412104118270" style="zoom:50%;">
</li>
<li><p>检验比较：</p>
<ul>
<li>如果欧氏距离越小，例如 $10^{-7}$，甚至更小，则表明 $d\theta_{approx}$ 与 $d\theta$ 越接近，即反向梯度计算是正确的，没有 bugs。</li>
<li>如果欧氏距离较大，例如 $10^{-5}$，则表明梯度计算可能出现问题，需要再次检查是否有 bugs 存在。</li>
<li>如果欧氏距离很大，例如 $10^{-3}$，甚至更大，则表明 $d\theta_{approx}$ 与 $d\theta$ 差别很大，梯度下降计算过程有 bugs，需要仔细检查。</li>
</ul>
</li>
</ul>
<h3 id="Gradient-Checking-Implementation-Notes"><a href="#Gradient-Checking-Implementation-Notes" class="headerlink" title="Gradient Checking Implementation Notes"></a><strong>Gradient Checking Implementation Notes</strong></h3><ul>
<li><p>不要在整个训练过程中都进行梯度检查，仅仅作为debug使用。</p>
</li>
<li><p>如果梯度检查出现错误，找到对应出错的梯度，检查其推导是否出现错误。</p>
</li>
<li><p>注意不要忽略正则化项，计算近似梯度的时候要包括进去。</p>
</li>
<li><p>梯度检查时关闭dropout，检查完毕后再打开dropout。</p>
</li>
<li><p>随机初始化时运行梯度检查，经过一些训练后再进行梯度检查（不常用）。</p>
</li>
</ul>
<h2 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h2><blockquote>
<p>建议看看这个<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1oY411N7Xz/?spm_id_from=333.337.search-card.all.click&vd_source=8e348c918e9f7eca8805bd4e1debb74a">视频</a></p>
</blockquote>
<h3 id="Mini-batch-gradient-descent"><a href="#Mini-batch-gradient-descent" class="headerlink" title="Mini-batch gradient descent"></a><strong>Mini-batch gradient descent</strong></h3><img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412160212366.png" alt="image-20230412160212366" style="zoom:50%;">

<ol>
<li><p>Batch Gradient Descent（批量梯度下降）的缺点：对于大量训练样本，计算梯度需要遍历所有样本，速度慢。</p>
</li>
<li><p>Mini-batch Gradient Descent（小批量梯度下降）：将训练样本分成若干个子集（mini-batches），每个子集包含少量样本（例如1000），在每个子集上进行梯度下降，速度更快。</p>
</li>
<li><p>训练过程：将总的训练样本分成T个子集（mini-batches），对每个mini-batch进行神经网络训练，包括Forward Propagation、Compute Cost Function、Backward Propagation，最后对权重和偏置进行更新。</p>
</li>
<li><p>一个epoch（训练周期）：对于Batch Gradient Descent，一个epoch只进行一次梯度下降算法；而Mini-Batches Gradient Descent，一个epoch会进行T次梯度下降算法。</p>
</li>
<li><p>多次epoch训练：可以进行多次epoch训练，每次epoch最好将总体训练数据重新打乱、重新分成T组mini-batches，这样有利于训练出最佳的神经网络模型。</p>
</li>
</ol>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412160329067.png" alt="image-20230412160329067" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412160341046.png" alt="image-20230412160341046" style="zoom:50%;">

<ol start="6">
<li>符号上标含义：</li>
</ol>
<ul>
<li>X(i)：第i个样本</li>
<li>Z[l]：神经网络第l层网络的线性输出</li>
<li>X{t}：第t组mini-batch</li>
<li>Y{t}：第t组mini-batch的输出</li>
</ul>
<h3 id="Understanding-mini-batch-gradient-descent"><a href="#Understanding-mini-batch-gradient-descent" class="headerlink" title="Understanding mini-batch gradient descent"></a><strong>Understanding mini-batch gradient descent</strong></h3><ol>
<li>Batch gradient descent会使cost不断减小，而Mini-batch gradient descent的cost可能出现细微振荡，但整体趋势是下降的，最终也能得到较低的cost值。</li>
<li>Mini-batch gradient descent中不同mini-batch之间是有差异的，这是细微振荡的原因。</li>
<li>每个mini-batch的大小应该如何选择？如果mini-batch size&#x3D;m，即为Batch gradient descent，只包含一个子集为(X{1},Y{1})&#x3D;(X,Y)；如果mini-batch size&#x3D;1，即为Stachastic gradient descent，每个样本就是一个子集(X{1},Y{1})&#x3D;(x(i),y(i))，共有m个子集。推荐常用的mini-batch size为64,128,256,512。</li>
<li>Batch gradient descent使用所有m个样本，每次前进的速度有些慢。Stachastic gradient descent每次前进速度很快，但路线曲折，有较大的振荡。mini-batch gradient descent相当于结合了Batch gradient descent和Stachastic gradient descent各自的优点，既能使用向量化优化算法，又能快速地找到最小值。</li>
<li>如果总体样本数量m≤2000，建议直接使用Batch gradient descent。如果总体样本数量m很大时，建议将样本分成许多mini-batches。</li>
</ol>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412161524341.png" alt="image-20230412161524341" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412161534208.png" alt="image-20230412161534208" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412161542195.png" alt="image-20230412161542195" style="zoom:50%;">

<h3 id="Exponentially-weighted-averages-指数加权平均值"><a href="#Exponentially-weighted-averages-指数加权平均值" class="headerlink" title="Exponentially weighted averages(指数加权平均值)"></a><strong>Exponentially weighted averages</strong>(指数加权平均值)</h3><h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412161940206.png" alt="image-20230412161940206" style="zoom:50%;">

<p>​												$\Downarrow$</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412162308435.png" alt="image-20230412162308435" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412162325776.png" alt="image-20230412162325776" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412162332885.png" alt="image-20230412162332885" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412162342006.png" alt="image-20230412162342006" style="zoom:50%;">

<h4 id="迭代次数"><a href="#迭代次数" class="headerlink" title="迭代次数"></a>迭代次数</h4><p>$\frac{1}{1-\beta}$</p>
<p>原因：</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412162454191.png" alt="image-20230412162454191" style="zoom:50%;">

<h3 id="Understanding-exponetially-weighted-averages"><a href="#Understanding-exponetially-weighted-averages" class="headerlink" title="Understanding exponetially weighted averages"></a><strong>Understanding exponetially weighted averages</strong></h3><h4 id="公式写下来"><a href="#公式写下来" class="headerlink" title="公式写下来"></a>公式写下来</h4><img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412162701714.png" alt="image-20230412162701714" style="zoom:50%;">

<blockquote>
<p>将原始数据值与衰减指数点乘，相当于做了指数衰减，离得越近，影响越大，离得越远，影响越小，衰减越厉害</p>
</blockquote>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412162722004.png" alt="image-20230412162722004" style="zoom: 33%;">

<h4 id="代码怎么写节省内存"><a href="#代码怎么写节省内存" class="headerlink" title="代码怎么写节省内存"></a>代码怎么写节省内存</h4><img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412162746241.png" alt="image-20230412162746241" style="zoom:50%;">

<h3 id="Bias-correction-in-exponentially-weighted-average-指数加权平均偏差校正"><a href="#Bias-correction-in-exponentially-weighted-average-指数加权平均偏差校正" class="headerlink" title="Bias correction in exponentially weighted average(指数加权平均偏差校正)"></a><strong>Bias correction in exponentially weighted average</strong>(指数加权平均偏差校正)</h3><h4 id="计算式"><a href="#计算式" class="headerlink" title="计算式"></a>计算式</h4><p>$$<br>\frac{V_t}{1-\beta^t}<br>$$</p>
<h4 id="不是必须"><a href="#不是必须" class="headerlink" title="不是必须"></a>不是必须</h4><p>这个矫正不是必须的，后面影响不大</p>
<h3 id="Gradient-descent-with-momentum（上面引出的算法）"><a href="#Gradient-descent-with-momentum（上面引出的算法）" class="headerlink" title="Gradient descent with momentum（上面引出的算法）"></a><strong>Gradient descent with momentum</strong>（上面引出的算法）</h3><img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412164156193.png" alt="image-20230412164156193" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412164207320.png" alt="image-20230412164207320" style="zoom:50%;">

<p>有的文献</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412164258943.png" alt="image-20230412164258943" style="zoom:50%;">

<h3 id="RMSprop（另外一种优化梯度下降速度的算法）"><a href="#RMSprop（另外一种优化梯度下降速度的算法）" class="headerlink" title="RMSprop（另外一种优化梯度下降速度的算法）"></a><strong>RMSprop</strong>（另外一种优化梯度下降速度的算法）</h3><img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412165251889.png" alt="image-20230412165251889" style="zoom:50%;">

<p>图像</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412165304120.png" alt="image-20230412165304120" style="zoom:50%;">

<p>为了避免RMSprop算法中分母为零，通常可以在分母增加一个极小的常数</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412165323407.png" alt="image-20230412165323407" style="zoom:50%;">

<p>比如$\varepsilon&#x3D;10^{-8}$</p>
<h3 id="Adam-optimization-algorithm"><a href="#Adam-optimization-algorithm" class="headerlink" title="Adam optimization algorithm"></a><strong>Adam optimization algorithm</strong></h3><img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412165635202.png" alt="image-20230412165635202" style="zoom:50%;">

<h3 id="Learning-rate-decay"><a href="#Learning-rate-decay" class="headerlink" title="Learning rate decay"></a><strong>Learning rate decay</strong></h3><p>学习因子随着迭代慢慢减小可以使得训练加速</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412165858036.png" alt="image-20230412165858036" style="zoom:50%;">

<p>计算公式有很多可以选</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412165920307.png" alt="image-20230412165920307" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412165927991.png" alt="image-20230412165927991" style="zoom:50%;">

<h3 id="The-problem-of-local-optima"><a href="#The-problem-of-local-optima" class="headerlink" title="The problem of local optima"></a><strong>The problem of local optima</strong></h3><img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230412170242234.png" alt="image-20230412170242234" style="zoom:50%;">

<p>右边的形状（马鞍状）可以大大降低训练速度</p>
<p>上面的优化算法可以有效的减少这种情况的发生</p>
<ul>
<li><strong>只要选择合理的强大的神经网络，一般不太可能陷入local optima</strong></li>
<li><strong>Plateaus可能会使梯度下降变慢，降低学习速度</strong></li>
</ul>
<h2 id="超参数调试、Batch正则化和编程框架"><a href="#超参数调试、Batch正则化和编程框架" class="headerlink" title="超参数调试、Batch正则化和编程框架"></a>超参数调试、Batch正则化和编程框架</h2><h3 id="Tuning-Process"><a href="#Tuning-Process" class="headerlink" title="Tuning Process"></a><strong>Tuning Process</strong></h3><p>主要是随机采样（有些超参数是可以均匀采样的）</p>
<p>一般来说超参数的选取采用随机采样，均匀采样不太行</p>
<p>还可以进行更密集的随机采样</p>
<blockquote>
<p>采样了一个方框之后，再对这个方框里的内容进行随机采样，也就是细化的过程</p>
</blockquote>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413170911744.png" alt="image-20230413170911744" style="zoom:50%;">

<h3 id="Using-an-appropriate-scale-to-pick-hyperparameters"><a href="#Using-an-appropriate-scale-to-pick-hyperparameters" class="headerlink" title="Using an appropriate scale to pick hyperparameters"></a><strong>Using an appropriate scale to pick hyperparameters</strong></h3><p>linear scale 和 log scale </p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413171238636.png" alt="image-20230413171238636" style="zoom:50%;">

<p>进行log的运算（这里就是数学里的lg）</p>
<p>python语句</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = np.log10(a)</span><br><span class="line">n = np.log10(b)</span><br><span class="line">r = np.random.rand()</span><br><span class="line">r = m + (n-m)*r</span><br><span class="line">r = np.power(<span class="number">10</span>,r)</span><br></pre></td></tr></table></figure>

<p>这种log随机采样用于$\alpha$和$\beta$</p>
<h3 id="Hyperparameters-tuning-in-practice-Pandas-vs-Caviar"><a href="#Hyperparameters-tuning-in-practice-Pandas-vs-Caviar" class="headerlink" title="Hyperparameters tuning in practice: Pandas vs. Caviar"></a><strong>Hyperparameters tuning in practice: Pandas vs. Caviar</strong></h3><ul>
<li>训练一个模型Panda approach</li>
<li>同时训练多个模型Caviar approach（每个模型不同的参数）</li>
</ul>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413172548338.png" alt="image-20230413172548338" style="zoom:50%;">

<h3 id="Normalizing-activations-in-a-network"><a href="#Normalizing-activations-in-a-network" class="headerlink" title="Normalizing activations in a network"></a><strong>Normalizing activations in a network</strong></h3><img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413174215113.png" alt="image-20230413174215113" style="zoom:50%;">

<p>之气是对输入进行batch Normalizationo</p>
<p>现在对中间输入进行batch Normalization</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413174319016.png" alt="image-20230413174319016" style="zoom:50%;">

<p>最后做一个线性调整</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413174348082.png" alt="image-20230413174348082" style="zoom:50%;">

<blockquote>
<p>$γ$和$β$是learnable parameters，类似于$W$和$b$一样，可以通过梯度下降等算法求得</p>
</blockquote>
<p>从激活函数的角度来说，如果各隐藏层的输入均值在靠近0的区域即处于激活函数的线性区域，这样不利于训练好的非线性神经网络，得到的模型效果也不会太好,所以进行线性调整</p>
<h3 id="Fitting-Batch-Norm-into-a-neural-network"><a href="#Fitting-Batch-Norm-into-a-neural-network" class="headerlink" title="Fitting Batch Norm into a neural network"></a><strong>Fitting Batch Norm into a neural network</strong></h3><img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413175009660.png" alt="image-20230413175009660" style="zoom:50%;">

<h4 id="b-i-的省略"><a href="#b-i-的省略" class="headerlink" title="$b^{[i]}$的省略"></a>$b^{[i]}$的省略</h4><p>个隐藏层去了均值，就不需要再优化$b^{[i]}$了</p>
<p>就如$1,2,3,4,5$和$3,4,5,6,7$归一化结果是相同的</p>
<h3 id="Why-does-Batch-Norm-work"><a href="#Why-does-Batch-Norm-work" class="headerlink" title="Why does Batch Norm work"></a><strong>Why does Batch Norm work</strong></h3><ul>
<li>减少各层$W$和$B$的耦合</li>
<li>从而增加鲁棒性</li>
<li>轻微的[正则化](#Basic Recipe for Machine Learning)效果</li>
</ul>
<h3 id="Batch-Norm-at-test-time"><a href="#Batch-Norm-at-test-time" class="headerlink" title="Batch Norm at test time"></a><strong>Batch Norm at test time</strong></h3><p>测试时的$\mu$和$\sigma^2$</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413182643567.png" alt="image-20230413182643567" style="zoom:50%;">

<ul>
<li><p>使用指数加权平均</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413182909608.png" alt="image-20230413182909608" style="zoom:50%;"></li>
</ul>
<h3 id="Softmax-Regression（Softmax回归）"><a href="#Softmax-Regression（Softmax回归）" class="headerlink" title="Softmax Regression（Softmax回归）"></a><strong>Softmax Regression</strong>（Softmax回归）</h3><p>简介</p>
<blockquote>
<p>Softmax回归（Softmax Regression）是一种分类算法，用于将输入向量映射到多个类别中的一个概率分布。</p>
<p>它是逻辑回归的一种扩展，可以处理多类分类问题，例如将一组输入特征预测为多个输出类别之一。 在Softmax回归中，对于每个类别，计算输入特征的加权和，并将其应用于softmax函数以获得类别的概率分布。softmax函数将每个类别的加权和作为指数函数的指数，然后将所有结果除以它们的总和，以确保概率分布总和为1。模型将输出概率最高的类别作为预测结果。</p>
<p>在训练期间，使用交叉熵损失函数来测量模型预测与真实标签之间的差异，并使用梯度下降优化算法最小化损失。通过反复迭代，模型可以学习如何预测正确的类别，并对新的输入进行分类。</p>
<p>Softmax回归是一种常见的机器学习算法，常用于图像识别、语音识别、自然语言处理等领域的多类别分类问题。</p>
</blockquote>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413183546134.png" alt="image-20230413183546134" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413184010280.png" alt="image-20230413184010280" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413184018474.png" alt="image-20230413184018474" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413184028303.png" alt="image-20230413184028303" style="zoom:50%;">

<p>例子</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413184046074.png" alt="image-20230413184046074" style="zoom:50%;">

<h3 id="Training-a-softmax-classifier"><a href="#Training-a-softmax-classifier" class="headerlink" title="Training a softmax classifier"></a><strong>Training a softmax classifier</strong></h3><img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413190807045.png" alt="image-20230413190807045" style="zoom:50%;">

<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413190941182.png" alt="image-20230413190941182" style="zoom:50%;">

<p>梯度下降算法</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413191050896.png" alt="image-20230413191050896" style="zoom:50%;">

<p>结果没变</p>
<img src="/2023/04/02/%E4%BC%98%E5%8C%96%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20230413191103844.png" alt="image-20230413191103844" style="zoom:50%;">

<h3 id="框架略"><a href="#框架略" class="headerlink" title="框架略"></a>框架略</h3>
    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="DragonGong 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="DragonGong 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/04/02/hello-world/" rel="prev" title="leetcode算法笔记">
      <i class="fa fa-chevron-left"></i> leetcode算法笔记
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/04/04/java%E8%AF%AD%E6%B3%95%E5%AD%A6%E4%B9%A0/" rel="next" title="java语法学习">
      java语法学习 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2"><span class="nav-number">1.</span> <span class="nav-text">深度学习的实用层面</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Train-x2F-Dev-x2F-Test-sets"><span class="nav-number">1.1.</span> <span class="nav-text">Train&#x2F;Dev&#x2F;Test sets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bias-x2F-Variance-%E5%81%8F%E5%B7%AE%EF%BC%88Bias%EF%BC%89%E5%92%8C%E6%96%B9%E5%B7%AE%EF%BC%88Variance%EF%BC%89"><span class="nav-number">1.2.</span> <span class="nav-text">Bias&#x2F;Variance(偏差（Bias）和方差（Variance）)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Basic-Recipe-for-Machine-Learning"><span class="nav-number">1.3.</span> <span class="nav-text">Basic Recipe for Machine Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization"><span class="nav-number">1.4.</span> <span class="nav-text">Regularization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#L1%E5%92%8CL2%E8%8C%83%E6%95%B0"><span class="nav-number">1.4.1.</span> <span class="nav-text">L1和L2范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E5%8F%82%E6%95%B0-lambda"><span class="nav-number">1.4.2.</span> <span class="nav-text">正则化参数$\lambda$</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-regularization-reduces-overfitting"><span class="nav-number">1.5.</span> <span class="nav-text">Why regularization reduces overfitting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout-Regularization"><span class="nav-number">1.6.</span> <span class="nav-text">Dropout Regularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Understanding-Dropout"><span class="nav-number">1.7.</span> <span class="nav-text">Understanding Dropout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-regularization-methods"><span class="nav-number">1.8.</span> <span class="nav-text">Other regularization methods</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#L2-regularization%E5%92%8Cdropout-regularization%E4%BB%A5%E5%A4%96%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">1.8.1.</span> <span class="nav-text">L2 regularization和dropout regularization以外的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Early-stopping%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="nav-number">1.8.2.</span> <span class="nav-text">Early stopping的缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L2-regularization%E7%9B%B8%E6%AF%94early-stopping%E7%9A%84%E4%BC%98%E7%82%B9"><span class="nav-number">1.8.3.</span> <span class="nav-text">L2 regularization相比early stopping的优点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Normalizing-inputs%EF%BC%88%E5%BD%92%E4%B8%80%E5%8C%96%E8%BE%93%E5%85%A5%EF%BC%89"><span class="nav-number">1.9.</span> <span class="nav-text">Normalizing inputs（归一化输入）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vanishing-and-Exploding-gradients-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="nav-number">1.10.</span> <span class="nav-text">Vanishing and Exploding gradients(梯度消失)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Weight-Initialization-for-Deep-Networks"><span class="nav-number">1.11.</span> <span class="nav-text">Weight Initialization for Deep Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Numerical-approximation-of-gradients"><span class="nav-number">1.12.</span> <span class="nav-text">Numerical approximation of gradients</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-checking"><span class="nav-number">1.13.</span> <span class="nav-text">Gradient checking</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-Checking-Implementation-Notes"><span class="nav-number">1.14.</span> <span class="nav-text">Gradient Checking Implementation Notes</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mini-batch-gradient-descent"><span class="nav-number">2.1.</span> <span class="nav-text">Mini-batch gradient descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Understanding-mini-batch-gradient-descent"><span class="nav-number">2.2.</span> <span class="nav-text">Understanding mini-batch gradient descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Exponentially-weighted-averages-%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E5%80%BC"><span class="nav-number">2.3.</span> <span class="nav-text">Exponentially weighted averages(指数加权平均值)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AE%97%E6%B3%95"><span class="nav-number">2.3.1.</span> <span class="nav-text">算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%AD%E4%BB%A3%E6%AC%A1%E6%95%B0"><span class="nav-number">2.3.2.</span> <span class="nav-text">迭代次数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Understanding-exponetially-weighted-averages"><span class="nav-number">2.4.</span> <span class="nav-text">Understanding exponetially weighted averages</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%85%AC%E5%BC%8F%E5%86%99%E4%B8%8B%E6%9D%A5"><span class="nav-number">2.4.1.</span> <span class="nav-text">公式写下来</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E6%80%8E%E4%B9%88%E5%86%99%E8%8A%82%E7%9C%81%E5%86%85%E5%AD%98"><span class="nav-number">2.4.2.</span> <span class="nav-text">代码怎么写节省内存</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bias-correction-in-exponentially-weighted-average-%E6%8C%87%E6%95%B0%E5%8A%A0%E6%9D%83%E5%B9%B3%E5%9D%87%E5%81%8F%E5%B7%AE%E6%A0%A1%E6%AD%A3"><span class="nav-number">2.5.</span> <span class="nav-text">Bias correction in exponentially weighted average(指数加权平均偏差校正)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%BC%8F"><span class="nav-number">2.5.1.</span> <span class="nav-text">计算式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8D%E6%98%AF%E5%BF%85%E9%A1%BB"><span class="nav-number">2.5.2.</span> <span class="nav-text">不是必须</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-descent-with-momentum%EF%BC%88%E4%B8%8A%E9%9D%A2%E5%BC%95%E5%87%BA%E7%9A%84%E7%AE%97%E6%B3%95%EF%BC%89"><span class="nav-number">2.6.</span> <span class="nav-text">Gradient descent with momentum（上面引出的算法）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RMSprop%EF%BC%88%E5%8F%A6%E5%A4%96%E4%B8%80%E7%A7%8D%E4%BC%98%E5%8C%96%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E9%80%9F%E5%BA%A6%E7%9A%84%E7%AE%97%E6%B3%95%EF%BC%89"><span class="nav-number">2.7.</span> <span class="nav-text">RMSprop（另外一种优化梯度下降速度的算法）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam-optimization-algorithm"><span class="nav-number">2.8.</span> <span class="nav-text">Adam optimization algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-rate-decay"><span class="nav-number">2.9.</span> <span class="nav-text">Learning rate decay</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-problem-of-local-optima"><span class="nav-number">2.10.</span> <span class="nav-text">The problem of local optima</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E8%B0%83%E8%AF%95%E3%80%81Batch%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E7%BC%96%E7%A8%8B%E6%A1%86%E6%9E%B6"><span class="nav-number">3.</span> <span class="nav-text">超参数调试、Batch正则化和编程框架</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tuning-Process"><span class="nav-number">3.1.</span> <span class="nav-text">Tuning Process</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Using-an-appropriate-scale-to-pick-hyperparameters"><span class="nav-number">3.2.</span> <span class="nav-text">Using an appropriate scale to pick hyperparameters</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hyperparameters-tuning-in-practice-Pandas-vs-Caviar"><span class="nav-number">3.3.</span> <span class="nav-text">Hyperparameters tuning in practice: Pandas vs. Caviar</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Normalizing-activations-in-a-network"><span class="nav-number">3.4.</span> <span class="nav-text">Normalizing activations in a network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Fitting-Batch-Norm-into-a-neural-network"><span class="nav-number">3.5.</span> <span class="nav-text">Fitting Batch Norm into a neural network</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#b-i-%E7%9A%84%E7%9C%81%E7%95%A5"><span class="nav-number">3.5.1.</span> <span class="nav-text">$b^{[i]}$的省略</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-does-Batch-Norm-work"><span class="nav-number">3.6.</span> <span class="nav-text">Why does Batch Norm work</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Batch-Norm-at-test-time"><span class="nav-number">3.7.</span> <span class="nav-text">Batch Norm at test time</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Softmax-Regression%EF%BC%88Softmax%E5%9B%9E%E5%BD%92%EF%BC%89"><span class="nav-number">3.8.</span> <span class="nav-text">Softmax Regression（Softmax回归）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-a-softmax-classifier"><span class="nav-number">3.9.</span> <span class="nav-text">Training a softmax classifier</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A1%86%E6%9E%B6%E7%95%A5"><span class="nav-number">3.10.</span> <span class="nav-text">框架略</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="DragonGong"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">DragonGong</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/CloudDragonGong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;CloudDragonGong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:dragongong20040318@gmail.com" title="E-Mail → mailto:dragongong20040318@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/DragonGong0318" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;DragonGong0318" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/20801014/%e9%be%9a%e4%ba%91%e9%be%99" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;users&#x2F;20801014&#x2F;%e9%be%9a%e4%ba%91%e9%be%99" rel="noopener" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i>StackOverflow</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DragonGong</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">84k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">1:16</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>



<br /> #换行
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("04/21/2019 15:54:40");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
