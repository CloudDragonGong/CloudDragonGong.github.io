<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.ico">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-material.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="本片笔记从是吴恩达教授的学习笔记，以及配套的博客梳理，方便巩固复习和完成编程作业。">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达深度学习教程">
<meta property="og:url" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/index.html">
<meta property="og:site_name" content="云龙博客">
<meta property="og:description" content="本片笔记从是吴恩达教授的学习笔记，以及配套的博客梳理，方便巩固复习和完成编程作业。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411195946029.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411200328890.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411203152258.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411210832437.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411210851178.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411211306029.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411211318770.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411232643691.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411234454503.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411234651570.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411234707571.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411235435598.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230412102844661.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230412102853117.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230412104055472.png">
<meta property="og:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230412104118270.png">
<meta property="article:published_time" content="2023-04-02T09:51:35.000Z">
<meta property="article:modified_time" content="2023-04-12T02:43:35.549Z">
<meta property="article:author" content="DragonGong">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411195946029.png">

<link rel="canonical" href="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>吴恩达深度学习教程 | 云龙博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">云龙博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="DragonGong">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="云龙博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          吴恩达深度学习教程
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-04-02 17:51:35" itemprop="dateCreated datePublished" datetime="2023-04-02T17:51:35+08:00">2023-04-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-04-12 10:43:35" itemprop="dateModified" datetime="2023-04-12T10:43:35+08:00">2023-04-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">吴恩达深度学习笔记</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>5 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>本片笔记从是<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1FT4y1E74V/?spm_id_from=333.999.0.0">吴恩达教授</a>的学习笔记，以及配套的<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_44543774/article/details/119154088?spm=1001.2014.3001.5506">博客</a>梳理，方便巩固复习和完成编程作业。</p>
</blockquote>
<span id="more"></span>

<h2 id="深度学习的实用层面"><a href="#深度学习的实用层面" class="headerlink" title="深度学习的实用层面"></a>深度学习的实用层面</h2><h3 id="Train-x2F-Dev-x2F-Test-sets"><a href="#Train-x2F-Dev-x2F-Test-sets" class="headerlink" title="Train&#x2F;Dev&#x2F;Test sets"></a>Train&#x2F;Dev&#x2F;Test sets</h3><blockquote>
<p>训练集（Training sets）、验证集（Development sets）、测试集（Test sets）</p>
</blockquote>
<img src="/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411195946029.png" alt="image-20230411195946029" style="zoom:50%;">

<p>样本数据分成三个部分：Train&#x2F;Dev&#x2F;Test sets</p>
<ul>
<li>本数量不是很大的情况下，例如100,1000,10000,Train&#x2F;Dev&#x2F;Test sets的比例通常可以设置为比例为<code>60%、20%、20%</code></li>
<li>大数据样本，Train&#x2F;Dev&#x2F;Test sets的比例通常可以设置为<code>98%/1%/1%</code></li>
</ul>
<h3 id="Bias-x2F-Variance-偏差（Bias）和方差（Variance）"><a href="#Bias-x2F-Variance-偏差（Bias）和方差（Variance）" class="headerlink" title="Bias&#x2F;Variance(偏差（Bias）和方差（Variance）)"></a>Bias&#x2F;Variance(偏差（Bias）和方差（Variance）)</h3><img src="/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411200328890.png" alt="image-20230411200328890" style="zoom:50%;">

<img src="/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411203152258.png" alt="image-20230411203152258" style="zoom: 33%;">

<ul>
<li>Train set error为1%，而Dev set error为11%–&gt;过拟合</li>
<li>Train set error为15%，而Dev set error为16% –&gt; 欠拟合</li>
<li>Train set error为15%，而Dev set error为30% –&gt; 又过拟合又欠拟合（最坏的情况）</li>
<li>假设Train set error为0.5%，而Dev set error为1% –&gt; 最好的情况</li>
</ul>
<blockquote>
<p>模型既存在high bias也存在high variance，可以理解成某段区域是欠拟合的，某段区域是过拟合的</p>
</blockquote>
<h3 id="Basic-Recipe-for-Machine-Learning"><a href="#Basic-Recipe-for-Machine-Learning" class="headerlink" title="Basic Recipe for Machine Learning"></a>Basic Recipe for Machine Learning</h3><ul>
<li><p>如何解决欠拟合</p>
<blockquote>
<p>增加神经网络的层数和节点数、选择更复杂的模型、增加训练时间等方式</p>
</blockquote>
</li>
<li><p>如何解决过拟合</p>
<blockquote>
<p>加训练集数据、使用正则化技术等方式</p>
</blockquote>
</li>
</ul>
<p>为什么深度学习如此强大：</p>
<blockquote>
<p>传统机器学习算法中偏差和方差通常是对立的，减小偏差会增加方差，减小方差会增加偏差。</p>
<p>但是在深度学习中，通过使用更复杂的神经网络和大量的训练数据，我们一般可以同时减小偏差和方差</p>
</blockquote>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><h4 id="L1和L2范数"><a href="#L1和L2范数" class="headerlink" title="L1和L2范数"></a>L1和L2范数</h4><p>L1范数和L2范数是衡量向量大小的方式。</p>
<p>L1范数，也称为曼哈顿距离（Manhattan distance），是指向量各个元素绝对值之和。对于一个n维向量x，它的L1范数为 ||x||1 &#x3D; |x1| + |x2| + … + |xn|。</p>
<p>L2范数，也称为欧几里得距离（Euclidean distance），是指向量各个元素的平方和再开平方根。对于一个n维向量x，它的L2范数为 ||x||2 &#x3D; sqrt(x1^2 + x2^2 + … + xn^2)。</p>
<p>L1范数和L2范数都可以用于正则化（regularization）技术中，其中L1正则化通常会导致模型的系数变得稀疏（即很多系数为0），而L2正则化则会让模型的系数尽可能地小，但不会稀疏。在神经网络中，L2正则化也被称为权重衰减（weight decay）。</p>
<h4 id="正则化参数-lambda"><a href="#正则化参数-lambda" class="headerlink" title="正则化参数$\lambda$"></a>正则化参数$\lambda$</h4><p>在正则化中，超参数λ（也称为正则化参数或惩罚因子）是用于控制正则化强度的调整参数。较大的λ值会导致更强的正则化，可以有效减少模型的过拟合问题，但可能会增加模型的欠拟合问题。较小的λ值会导致较弱的正则化，使模型更加复杂，可能会导致过拟合问题</p>
<p>通过调整λ的值来达到合适的正则化强度，一般采用<em>交叉验证</em>的方法来确定最佳的λ值</p>
<img src="/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411210832437.png" alt="image-20230411210832437" style="zoom:50%;">

<img src="/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411210851178.png" alt="image-20230411210851178" style="zoom:50%;">

<h3 id="Why-regularization-reduces-overfitting"><a href="#Why-regularization-reduces-overfitting" class="headerlink" title="Why regularization reduces overfitting"></a><strong>Why regularization reduces overfitting</strong></h3><img src="/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411211306029.png" alt="image-20230411211306029" style="zoom:50%;">

<img src="/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411211318770.png" alt="image-20230411211318770" style="zoom:50%;">

<blockquote>
<p>正则化能够减少过拟合的原因是，它通过对模型的参数进行约束，降低了模型的复杂度，从而减少了模型在训练数据上的误差，并提高了模型的泛化能力。具体来说，L1和L2正则化会对模型的参数进行惩罚，使得模型在优化过程中更倾向于选择较小的参数值，从而达到减小模型复杂度的效果。通过减小模型的复杂度，模型在验证数据上的表现也会得到提升，从而有效避免过拟合的问题。</p>
</blockquote>
<h3 id="Dropout-Regularization"><a href="#Dropout-Regularization" class="headerlink" title="Dropout Regularization"></a><strong>Dropout Regularization</strong></h3><ul>
<li><p>什么是<strong>Dropout Regularization</strong></p>
<blockquote>
<p>Dropout是指在深度学习网络的训练过程中，对于每层的神经元，按照一定的概率将其暂时从网络中丢弃。也就是说，每次训练时，每一层都有部分神经元不工作，起到简化复杂网络模型的效果，从而避免发生过拟合。</p>
</blockquote>
</li>
</ul>
<img src="/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411232643691.png" alt="image-20230411232643691" style="zoom:50%;">

<ul>
<li><p>如何实现</p>
<blockquote>
<p>Dropout有不同的实现方法，接下来介绍一种常用的方法：Inverted dropout。假设对于第$l$层神经元，设定保留神经元比例概率keep_prob&#x3D;0.8，即该层有20%的神经元停止工作。$dl$为dropout向量，设置$dl$为随机vector，其中80%的元素为1，20%的元素为0</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dl = np.random.rand(al.shape[<span class="number">0</span>],al.shape[<span class="number">1</span>])&lt;keep_prob</span><br><span class="line">al = np.multiply(al,dl)</span><br><span class="line">al /= keep_prob <span class="comment"># scale up</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>什么是scale up</p>
<blockquote>
<p>在Inverted dropout中，我们对某一层的神经元进行了随机的丢弃操作，即将某些神经元的输出值设为0。为了避免丢弃操作对网络的输出造成过大的影响，我们需要对剩下的神经元的输出值进行调整，即将剩下的神经元的输出值除以保留概率（即1-丢弃概率）。这个调整过程就叫做scale up（缩放）操作。</p>
<p>举个例子，如果我们对某一层的神经元进行了50%的丢弃，即丢弃概率为0.5，那么保留概率就是0.5，我们需要对剩下的神经元的输出值进行调整，即将其除以0.5，使其尽量保持不变。这样，在经过dropout后，剩下的神经元的输出值的均值和方差与没有进行dropout的情况下差不多，这有助于减轻dropout对网络输出的影响，从而提高网络的泛化能力。</p>
</blockquote>
</li>
<li><p>使用dropout训练结束后，在测试和实际应用模型时，<strong>不需要进行dropout和随机删减神经元</strong>，所有的神经元都在工作</p>
</li>
</ul>
<h3 id="Understanding-Dropout"><a href="#Understanding-Dropout" class="headerlink" title="Understanding Dropout"></a><strong>Understanding Dropout</strong></h3><ul>
<li><p>keep_prob 和 keep_out </p>
<blockquote>
<p>指代的是 dropout 算法中的概率值，表示神经元保留的概率，即被保留下来的概率。在实际使用中，keep_prob 和 keep_out 可以看作是同一个概念，表示的是相同的概率值。只是在不同的文献和资料中，可能使用的术语不同</p>
</blockquote>
</li>
<li><p>不同的隐藏层的dropout系数keep_prob可以不同。</p>
</li>
<li><p>神经元越多的隐藏层，keep_prob可以设置的小一些，例如0.5。</p>
</li>
<li><p>神经元越少的隐藏层，keep_prob可以设置的大一些，例如0.8，设置是1。</p>
</li>
<li><p>不建议对输入层进行dropout，如果输入层维度很大，例如图片，那么可以设置dropout，但keep_prob应设置的大一些，例如0.8，0.9。</p>
</li>
<li><p>越容易出现overfitting的隐藏层，其keep_prob就设置的相对小一些。</p>
</li>
<li><p>没有固定的做法，通常可以根据validation进行选择。# validation–验证，实验</p>
</li>
</ul>
<h3 id="Other-regularization-methods"><a href="#Other-regularization-methods" class="headerlink" title="Other regularization methods"></a><strong>Other regularization methods</strong></h3><h4 id="L2-regularization和dropout-regularization以外的方法"><a href="#L2-regularization和dropout-regularization以外的方法" class="headerlink" title="L2 regularization和dropout regularization以外的方法"></a>L2 regularization和dropout regularization以外的方法</h4><ul>
<li><p>增加训练样本数量，但通常成本较高，因此可以使用数据增强技术来制造更多样本，如图片翻转、旋转、缩放等，这些都能防止过拟合。</p>
</li>
<li><p>在数字识别中，也可以对原有图片进行任意旋转、扭曲或者添加噪声等操作。</p>
</li>
<li><p>另一种防止过拟合的方法是early stopping，即在训练过程中根据train set error和dev set error的变化趋势来选择合适的迭代次数，以避免过拟合。</p>
<img src="/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411234454503.png" alt="image-20230411234454503" style="zoom:50%;"></li>
</ul>
<h4 id="Early-stopping的缺点"><a href="#Early-stopping的缺点" class="headerlink" title="Early stopping的缺点"></a>Early stopping的缺点</h4><ul>
<li>Early stopping通过减少训练次数来防止过拟合，与优化cost function的目标彼此对立，缺乏正交化。</li>
<li>Early stopping将上述两个目标融合在一起，同时优化，但可能没有“分而治之”的效果好。</li>
</ul>
<h4 id="L2-regularization相比early-stopping的优点"><a href="#L2-regularization相比early-stopping的优点" class="headerlink" title="L2 regularization相比early stopping的优点"></a>L2 regularization相比early stopping的优点</h4><ul>
<li>L2 regularization可以实现“分而治之”的效果：迭代训练足够多，减小cost function，同时也能有效防止过拟合。</li>
<li>L2 regularization的缺点之一是最优的正则化参数λ的选择比较复杂。</li>
<li>Early stopping相比L2 regularization比较简单，但L2 regularization更加常用一些。</li>
</ul>
<h3 id="Normalizing-inputs（归一化输入）"><a href="#Normalizing-inputs（归一化输入）" class="headerlink" title="Normalizing inputs（归一化输入）"></a><strong>Normalizing inputs</strong>（归一化输入）</h3><p>在训练神经网络时，标准化输入可以提高训练的速度。标准化输入就是对训练数据集进行归一化的操作，即将原始数据减去其均值$μ$后，再除以其方差$σ$2：</p>
<img src="/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411234651570.png" alt="image-20230411234651570" style="zoom:50%;">

<img src="/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411234707571.png" alt="image-20230411234707571" style="zoom:50%;">

<ol>
<li>标准化输入可以提高神经网络训练的速度和准确度。</li>
<li>标准化输入是对训练数据集进行归一化的操作，即将原始数据减去其均值后，再除以其方差。</li>
<li>在训练集进行标准化处理后，测试集或在实际应用时，应使用同样的均值和方差进行标准化处理，保证训练集和测试集的标准化操作一致。</li>
<li>标准化输入的主要目的是让所有输入在同样的尺度上归一化，方便进行梯度下降算法时能够更快更准确地找到全局最优解。</li>
<li>标准化处理可以让特征分布均匀，从而使得得到的cost function与权重和偏置的关系是类似圆形碗，便于梯度下降算法的优化。</li>
<li>如果输入特征之间的范围本来就比较接近，那么不进行标准化操作也是没有太大影响的，但标准化处理在大多数场合下还是值得推荐的</li>
</ol>
<h3 id="Vanishing-and-Exploding-gradients-梯度消失"><a href="#Vanishing-and-Exploding-gradients-梯度消失" class="headerlink" title="Vanishing and Exploding gradients(梯度消失)"></a><strong>Vanishing and Exploding gradients</strong>(梯度消失)</h3><img src="/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230411235435598.png" alt="image-20230411235435598" style="zoom:50%;">

<p>深度神经网络中存在梯度消失和梯度爆炸问题，导致训练过程困难。当各层权重都大于1或小于1时，激活函数的输出会随着层数的增加呈指数型增大或减小，出现数值爆炸或消失。同样，梯度也会出现同样的指数型增大或减小的变化，当网络层数很大时，如L&#x3D;150，则梯度会非常大或非常小，引起每次更新的步进长度过大或者过小，让训练过程十分困难。</p>
<h3 id="Weight-Initialization-for-Deep-Networks"><a href="#Weight-Initialization-for-Deep-Networks" class="headerlink" title="Weight Initialization for Deep Networks"></a><strong>Weight Initialization for Deep Networks</strong></h3><p>在深度神经网络中，初始化权重的方法对于网络的训练非常重要。在神经网络中，权重矩阵W的大小决定了神经元输出Z的大小。如果W过大或过小，神经元输出Z就容易出现梯度消失或梯度爆炸的问题，导致网络难以收敛。</p>
<p>针对这个问题，可以通过调整权重矩阵W的初始化方法来解决。</p>
<ul>
<li><p>对于使用tanh作为激活函数的网络，通常建议使用方差为1&#x2F;n的初始化方法；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l-<span class="number">1</span>])*np.sqrt(<span class="number">1</span>/n[l-<span class="number">1</span>]) </span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义神经网络结构</span></span><br><span class="line">layer_dims = [<span class="number">4</span>, <span class="number">5</span>, <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化权重</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initialize_weights</span>(<span class="params">layer_dims</span>):</span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    L = <span class="built_in">len</span>(layer_dims)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, L):</span><br><span class="line">        parameters[<span class="string">&#x27;W&#x27;</span> + <span class="built_in">str</span>(l)] = np.random.randn(layer_dims[l], layer_dims[l-<span class="number">1</span>]) * np.sqrt(<span class="number">1</span>/layer_dims[l-<span class="number">1</span>])</span><br><span class="line">        parameters[<span class="string">&#x27;b&#x27;</span> + <span class="built_in">str</span>(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用函数进行初始化</span></span><br><span class="line">parameters = initialize_weights(layer_dims)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看初始化后的权重</span></span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> parameters.items():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;W&#x27;</span> <span class="keyword">in</span> key:</span><br><span class="line">        <span class="built_in">print</span>(key + <span class="string">&quot; shape: &quot;</span> + <span class="built_in">str</span>(value.shape))</span><br><span class="line">        <span class="built_in">print</span>(key + <span class="string">&quot; value: &quot;</span> + <span class="built_in">str</span>(value))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>输出结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">W1 shape: (5, 4)</span><br><span class="line">W1 value: [[ 1.78862847  0.43650985  0.09649747 -1.8634927 ]</span><br><span class="line"> [-0.2773882  -0.35475898 -0.08274148 -0.62700068]</span><br><span class="line"> [-0.04381817 -0.47721803 -1.31386475   0.88462238]</span><br><span class="line"> [ 0.88131804  1.70957306  0.05003364 -0.40467741]</span><br><span class="line"> [-0.54535995 -1.54647732  0.98236743 -1.10106763]]</span><br><span class="line">W2 shape: (3, 5)</span><br><span class="line">W2 value: [[-0.1382643   0.20345524  0.75878541 -0.22404411  0.22863013]</span><br><span class="line"> [ 0.44513761 -0.37598474 -0.21611069 -0.30847027 -0.10917586]</span><br><span class="line"> [ 0.90082649  0.46566244 -1.53624369  1.48825219  1.89588918]]</span><br><span class="line"></span><br></pre></td></tr></table></figure>


</li>
<li><p>对于使用ReLU作为激活函数的网络，建议使用方差为2&#x2F;n的初始化方法。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l-<span class="number">1</span>])*np.sqrt(<span class="number">2</span>/n[l-<span class="number">1</span>]) </span><br></pre></td></tr></table></figure>


</li>
<li><p>此外，Yoshua Bengio提出了一种初始化方法，可以设置权重的方差为2&#x2F;n[l-1]*n[l]，其中n[l-1]和n[l]分别是前一层和当前层的神经元数量。不同的初始化方法适用于不同的网络结构和激活函数，选择哪种方法要根据实际情况进行调整。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w[l] = np.random.randn(n[l],n[l-<span class="number">1</span>])*np.sqrt(<span class="number">2</span>/n[l-<span class="number">1</span>]*n[l]) </span><br></pre></td></tr></table></figure></li>
</ul>
<p>在实际应用中，为了获得最优的初始化方法，可以通过验证集对超参数进行调整。</p>
<h3 id="Numerical-approximation-of-gradients"><a href="#Numerical-approximation-of-gradients" class="headerlink" title="Numerical approximation of gradients"></a><strong>Numerical approximation of gradients</strong></h3><img src="/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230412102844661.png" alt="image-20230412102844661" style="zoom:50%;">

<img src="/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230412102853117.png" alt="image-20230412102853117" style="zoom:50%;">

<h3 id="Gradient-checking"><a href="#Gradient-checking" class="headerlink" title="Gradient checking"></a><strong>Gradient checking</strong></h3><p>检查步骤</p>
<ul>
<li><p>计算$J(\theta)$</p>
</li>
<li><p>计算$d\theta$:通过反向传播来计算</p>
</li>
<li><p>计算估计的$d\theta$</p>
<img src="/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230412104055472.png" alt="image-20230412104055472" style="zoom:50%;">
</li>
<li><p>计算欧氏距离比较：</p>
<img src="/2023/04/02/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%95%99%E7%A8%8B/image-20230412104118270.png" alt="image-20230412104118270" style="zoom:50%;">
</li>
<li><p>检验比较：</p>
<ul>
<li>如果欧氏距离越小，例如 $10^{-7}$，甚至更小，则表明 $d\theta_{approx}$ 与 $d\theta$ 越接近，即反向梯度计算是正确的，没有 bugs。</li>
<li>如果欧氏距离较大，例如 $10^{-5}$，则表明梯度计算可能出现问题，需要再次检查是否有 bugs 存在。</li>
<li>如果欧氏距离很大，例如 $10^{-3}$，甚至更大，则表明 $d\theta_{approx}$ 与 $d\theta$ 差别很大，梯度下降计算过程有 bugs，需要仔细检查。</li>
</ul>
</li>
</ul>
<h3 id="Gradient-Checking-Implementation-Notes"><a href="#Gradient-Checking-Implementation-Notes" class="headerlink" title="Gradient Checking Implementation Notes"></a><strong>Gradient Checking Implementation Notes</strong></h3><ul>
<li><p>不要在整个训练过程中都进行梯度检查，仅仅作为debug使用。</p>
</li>
<li><p>如果梯度检查出现错误，找到对应出错的梯度，检查其推导是否出现错误。</p>
</li>
<li><p>注意不要忽略正则化项，计算近似梯度的时候要包括进去。</p>
</li>
<li><p>梯度检查时关闭dropout，检查完毕后再打开dropout。</p>
</li>
<li><p>随机初始化时运行梯度检查，经过一些训练后再进行梯度检查（不常用）。</p>
</li>
</ul>

    </div>

    
    
    
        <div class="reward-container">
  <div></div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.png" alt="DragonGong 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.png" alt="DragonGong 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/04/02/hello-world/" rel="prev" title="leetcode算法笔记">
      <i class="fa fa-chevron-left"></i> leetcode算法笔记
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/04/04/java%E8%AF%AD%E6%B3%95%E5%AD%A6%E4%B9%A0/" rel="next" title="java语法学习">
      java语法学习 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9E%E7%94%A8%E5%B1%82%E9%9D%A2"><span class="nav-number">1.</span> <span class="nav-text">深度学习的实用层面</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Train-x2F-Dev-x2F-Test-sets"><span class="nav-number">1.1.</span> <span class="nav-text">Train&#x2F;Dev&#x2F;Test sets</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bias-x2F-Variance-%E5%81%8F%E5%B7%AE%EF%BC%88Bias%EF%BC%89%E5%92%8C%E6%96%B9%E5%B7%AE%EF%BC%88Variance%EF%BC%89"><span class="nav-number">1.2.</span> <span class="nav-text">Bias&#x2F;Variance(偏差（Bias）和方差（Variance）)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Basic-Recipe-for-Machine-Learning"><span class="nav-number">1.3.</span> <span class="nav-text">Basic Recipe for Machine Learning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization"><span class="nav-number">1.4.</span> <span class="nav-text">Regularization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#L1%E5%92%8CL2%E8%8C%83%E6%95%B0"><span class="nav-number">1.4.1.</span> <span class="nav-text">L1和L2范数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96%E5%8F%82%E6%95%B0-lambda"><span class="nav-number">1.4.2.</span> <span class="nav-text">正则化参数$\lambda$</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Why-regularization-reduces-overfitting"><span class="nav-number">1.5.</span> <span class="nav-text">Why regularization reduces overfitting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout-Regularization"><span class="nav-number">1.6.</span> <span class="nav-text">Dropout Regularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Understanding-Dropout"><span class="nav-number">1.7.</span> <span class="nav-text">Understanding Dropout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-regularization-methods"><span class="nav-number">1.8.</span> <span class="nav-text">Other regularization methods</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#L2-regularization%E5%92%8Cdropout-regularization%E4%BB%A5%E5%A4%96%E7%9A%84%E6%96%B9%E6%B3%95"><span class="nav-number">1.8.1.</span> <span class="nav-text">L2 regularization和dropout regularization以外的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Early-stopping%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="nav-number">1.8.2.</span> <span class="nav-text">Early stopping的缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#L2-regularization%E7%9B%B8%E6%AF%94early-stopping%E7%9A%84%E4%BC%98%E7%82%B9"><span class="nav-number">1.8.3.</span> <span class="nav-text">L2 regularization相比early stopping的优点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Normalizing-inputs%EF%BC%88%E5%BD%92%E4%B8%80%E5%8C%96%E8%BE%93%E5%85%A5%EF%BC%89"><span class="nav-number">1.9.</span> <span class="nav-text">Normalizing inputs（归一化输入）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Vanishing-and-Exploding-gradients-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1"><span class="nav-number">1.10.</span> <span class="nav-text">Vanishing and Exploding gradients(梯度消失)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Weight-Initialization-for-Deep-Networks"><span class="nav-number">1.11.</span> <span class="nav-text">Weight Initialization for Deep Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Numerical-approximation-of-gradients"><span class="nav-number">1.12.</span> <span class="nav-text">Numerical approximation of gradients</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-checking"><span class="nav-number">1.13.</span> <span class="nav-text">Gradient checking</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gradient-Checking-Implementation-Notes"><span class="nav-number">1.14.</span> <span class="nav-text">Gradient Checking Implementation Notes</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="DragonGong"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">DragonGong</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/CloudDragonGong" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;CloudDragonGong" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:dragongong20040318@gmail.com" title="E-Mail → mailto:dragongong20040318@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/DragonGong0318" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;DragonGong0318" rel="noopener" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/20801014/%e9%be%9a%e4%ba%91%e9%be%99" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;users&#x2F;20801014&#x2F;%e9%be%9a%e4%ba%91%e9%be%99" rel="noopener" target="_blank"><i class="fab fa-stack-overflow fa-fw"></i>StackOverflow</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DragonGong</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">70k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">1:04</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>



<br /> #换行
<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("04/21/2019 15:54:40");//此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
    }
setInterval("createtime()",250);
</script>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
